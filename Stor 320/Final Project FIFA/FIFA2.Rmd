---
title: "FIFA2 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(generics)
library(modelr)       #Helpful Functions in Modeling
library(xtable)
library(purrr)
library(broom)
library(class)
library(glmnet)
library(formattable)
```

```{r,include=FALSE}
table<-read.csv("FIFA_20.csv")
```

```{r,include=FALSE}
table2<-table%>%select("age":"sliding_tackle")
mod1=lm(composure~.,data=table2)
```

```{r,include=FALSE}
table3<-table2%>%select(-composure)
summary(mod1)
colnames(table3)[summary(mod1)$coefficients[-1,4]<0.01]


table3=table3%>%select(age,height_cm,shooting,passing,dribbling,defending,physic,fk_accuracy,stamina,penalties,standing_tackle,sliding_tackle)
table3=cbind(table3,composure=table2$composure)

```

```{r,include=FALSE}
mod<-function(data,i){
  mod1=lm(composure~poly(age,i)+poly(height_cm,i)+poly(shooting,i)+poly(passing,i)+poly(dribbling,i)+poly(defending,i)+poly(physic,i)+poly(fk_accuracy,i)+poly(stamina,i)+poly(penalties,i)+poly(standing_tackle,i)+poly(sliding_tackle,i),data=data)
  return (mod1)
}

table4=table3%>%crossv_kfold(10)

result<-table4%>%mutate(tr.model=map(train,mod,i=1))%>%
  mutate(predict=map2(test,tr.model,~augment(.y,newdata=.x)))%>%
  select(predict)%>%unnest(cols=c(predict))

rmse<-function(x,y){
  resid=x-y
  mse=mean(resid^2,na.rm=T)
  rmse=sqrt(mse)
  return(rmse)
}

mse<-function(x,y){
  resid=x-y
  mse=mean(resid^2,na.rm=T)
  return(mse)
}


mae<-function(x,y){
  resid=x-y
  return(mean(abs(resid)))
}


bias.func=function(x,y){
  res=x-y
  bias=mean(res,na.rm=T)
  return(bias)
}
```

```{r,include=FALSE}
array=rep(NA,12)
for (i in 1:12){
  result<-table4%>%mutate(tr.model=map(train,mod,i=i))%>%
  mutate(predict=map2(test,tr.model,~augment(.y,newdata=.x)))%>%
  select(predict)%>%unnest(cols=c(predict))
  array[i]=mae(result$composure,result$.fitted)
}
```


```{r,include=FALSE}
min_i=which.min(array)
min_i
ggplot(tibble(array))+geom_line(aes(1:12,array),color="lightskyblue2",size=2)+
  theme_minimal() +
  xlab("Choice of i") +
  ylab("MSE") +
  theme(text=element_text(size=10))
```

```{r,include=FALSE}
result<-table4%>%mutate(tr.model=map(train,mod,i=min_i))%>%
  mutate(predict=map2(test,tr.model,~augment(.y,newdata=.x)))%>%
  select(predict)%>%unnest(cols=c(predict))
result
```

```{r,include=FALSE}
table5=cbind(name=table$name,nation=table$nationality,club=table$club,table3,pred_composure=result$.fitted)
```

```{r,include=FALSE}
head(table5)
table5=table5%>%mutate(resid=pred_composure-composure)
```




```{r,include=FALSE}
DATA2<-table%>%select("age":"sliding_tackle")
y= DATA2$composure
X=model_matrix(DATA2,composure~.*.)[,-1]
var.names=names(X)
dim(X)
head(X)
```

```{r,include=FALSE}
set.seed(216)
cvmod.0=cv.glmnet(y=y,x=as.matrix(X),alpha=0)
set.seed(216)
cvmod.25=cv.glmnet(y=y,x=as.matrix(X),alpha=0.25)
set.seed(216)
cvmod.5=cv.glmnet(y=y,x=as.matrix(X),alpha=0.5)
set.seed(216)
cvmod.75=cv.glmnet(y=y,x=as.matrix(X),alpha=0.75)
set.seed(216)
cvmod.1=cv.glmnet(y=y,x=as.matrix(X),alpha=1)
cvmod.0
CV.0.ERROR=cvmod.0$cvm[which(cvmod.0$lambda==cvmod.0$lambda.1se)]
CV.25.ERROR=cvmod.25$cvm[which(cvmod.25$lambda==cvmod.25$lambda.1se)]
CV.5.ERROR=cvmod.5$cvm[which(cvmod.5$lambda==cvmod.5$lambda.1se)]
CV.75.ERROR=cvmod.75$cvm[which(cvmod.75$lambda==cvmod.75$lambda.1se)]
CV.1.ERROR=cvmod.1$cvm[which(cvmod.1$lambda==cvmod.1$lambda.1se)]

MOD.RESULT=tibble(alpha=c(0,0.25,0.5,0.75,1),
                  lambda=c(cvmod.0$lambda.1se,cvmod.25$lambda.1se,
                           cvmod.5$lambda.1se,cvmod.75$lambda.1se,
                           cvmod.1$lambda.1se),
                  CV.Error=c(CV.0.ERROR,CV.25.ERROR,CV.5.ERROR,
                             CV.75.ERROR,CV.1.ERROR))
print(MOD.RESULT)
```

```{r,include=FALSE}
best.alpha=MOD.RESULT$alpha[which.min(MOD.RESULT$CV.Error)]
best.lambda=MOD.RESULT$lambda[which.min(MOD.RESULT$CV.Error)]

best.mod=glmnet(y=y,x=as.matrix(X),nlambda=1,lambda=best.lambda,alpha=best.alpha)
best.coef=as.tibble(as.matrix(coef(best.mod)))
best.coef2=best.coef %>% 
              mutate(Parameter=c("Intercept",var.names)) %>%
              rename(Estimate=s0) %>%
              select(Parameter,Estimate)
nonzero.best.coef=best.coef2 %>%
                    filter(Estimate!=0)
print(nonzero.best.coef,n=1e3)



DATA2$composure.pred=predict(best.mod,newx=as.matrix(X))




```



Result

For question 1,"Which model and its variables best predict the variable composure." We look variables age, weight, height and other metrics assessing the skills of a player in total of 17 variables and we use two modeling techniques: Polynomial linear model and Shrinkage model. Since we want to simulate these two models in the same standards, we both fit them by using MSE
test to return the most accurate parameters. 

Polynomial linear model:
First we choose the variables according to their t values, We look at the variables with the probabiliy of t value falls under 0.01 to be signifiant. So we eventually have 12 variables out of 17. Then we use cross validation with 10 folds to split the data into 10 training and testing sets, and compute the predicted values. We also use for loops from 1 to 12 to choose the best exponent for all variables. And we compare them by using MSE test. It terms out power term 9 returns the lowest MSE. Then we apply the exponent to our method and return the list of predicted values for composure. 

Shrinkage Method:
Second, we try to use shrinkage method to predict composure. We first split the data into two matrices, one column maxtrix of composure and matrix of 17 variables and their interation. Then, we use set alpha to five values and apply cv.glmnet to find each alpha's lowest prediction error by MSE and its corresponding lambda. We compare these five results and choose the one with lowest MSE. It terms out the model is Lasso Regression with alpha equals to 1 and lambda equals 0.004. We use this model to choose the variables and predict the composure. It shrinks over 14000 variables into 70. And only choose 8 non-interacted variables out of 17. 

we also fit a scatter plot with actual values in x axis and predicted values in y axis. And a reference line. Both of models have points closely scattered around the reference line. 

```{r,echo=FALSE}
ggplot(result,aes(composure,.fitted))+geom_point()+geom_abline(slope=1,intercept=0,size=2,color="red",linetype="dashed")+ylab("Predicted composure")+theme_bw()+xlab("Actual Composure")

ggplot(DATA2) +
  geom_point(aes(x=composure,y=composure.pred)) +
  geom_abline(intercept=0,slope=1,linetype="dashed",size=2,color="red") +
  theme_bw() +
  ylab("Predicted Composure") +
  xlab("Actual Composure")



```

We also fit the frequency models for the absolute values of residuals. Two plots are very similar and both have their peak around 2 to 3. 

```{r ,warning=FALSE,echo=FALSE,message=FALSE}
ggplot(result,aes(abs(.fitted-composure)))+geom_freqpoly(color="lightskyblue2",size=2)+theme_bw()+ylab("Frequency")+xlab("Residuals")

ggplot(DATA2) +
  geom_freqpoly(aes(x=abs(composure.pred-composure)),color="lightskyblue2",size=2) +
  theme_bw() +
  xlab("Residuals") +
  ylab("Frequency")


```

Then we compare these two models by RMSE and MAE test, it terms out shinkage did a slightly better job in both of methods. The tibble quantitatively shows that these models are very similar in results. But we do have a bias in polynomial regression, since we set the power term on each variable to the same instead of running multiple for loops to find the best exponent for each variable. So the best predicting models might have lower prediction errors but it would be time-consuming for computer to result the best consequence and it does not consider the interaction terms of different variables. Rather, shrinkage model is more easy to generate and more accurate by considering the interaction variables. 

```{r,echo=FALSE}
rmsetable<-matrix(c(5.64,5.49,4.47,4.33), ncol=2, byrow=TRUE)
colnames(rmsetable)<-c("Polynomial Model","Shrinkage Model")
rownames(rmsetable)<-c("RMSE", "MAE")
kbl(rmsetable) %>%
  kable_styling(bootstrap_options = "striped", font_size = 7)

#tibble2<-data.frame(Polynomial_Model=c(round(rmse(result$composure,result$.fitted),2),                        round(mae(result$composure,result$.fitted),2)),                       Shrinkage_Model=c(round(rmse(DATA2$composure,DATA2$composure.pred),2),round(mae(DATA2$composure,DATA2$composure.pred),2)))
#rownames(tibble2)=c("RMSE","MAE")
#tibble2
```
----------------------------------------------------


